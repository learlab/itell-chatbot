{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321c1c28-21b4-4570-86f3-5a3184d84c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 17:25:37 llm_engine.py:72] Initializing an LLM engine with config: model='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-06 17:26:06 llm_engine.py:205] # GPU blocks: 1081, # CPU blocks: 327\n"
     ]
    }
   ],
   "source": [
    "from vllm import AsyncEngineArgs, AsyncLLMEngine\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model = \"Open-Orca/OpenOrcaxOpenChat-Preview2-13B\",\n",
    "    download_dir = None,\n",
    "    gpu_memory_utilization = 0.80,\n",
    ")\n",
    "               \n",
    "engine = AsyncLLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ae0801-5d07-4c82-8870-8aaf11657cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse, Response, StreamingResponse\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from vllm.utils import random_uuid\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "async def generate(request: Request) -> Response:\n",
    "    \"\"\"Generate completion for the request.\n",
    "\n",
    "    The request should be a JSON object with the following fields:\n",
    "    - prompt: the prompt to use for the generation.\n",
    "    - stream: whether to stream the results or not.\n",
    "    - other fields: the sampling parameters (See `SamplingParams` for details).\n",
    "    \"\"\"\n",
    "    request_dict = dict(request)\n",
    "    prompt = request_dict.pop(\"prompt\")\n",
    "    stream = request_dict.pop(\"stream\", False)\n",
    "    sampling_params = SamplingParams(**request_dict)\n",
    "    request_id = random_uuid()\n",
    "\n",
    "    results_generator = engine.generate(prompt, sampling_params, request_id)\n",
    "\n",
    "    # Streaming case\n",
    "    async def stream_results() -> AsyncGenerator[bytes, None]:\n",
    "        async for request_output in results_generator:\n",
    "            prompt = request_output.prompt\n",
    "            text_outputs = [\n",
    "                prompt + output.text for output in request_output.outputs\n",
    "            ]\n",
    "            ret = {\"text\": text_outputs}\n",
    "            yield (json.dumps(ret) + \"\\0\").encode(\"utf-8\")\n",
    "\n",
    "    if stream:\n",
    "        return StreamingResponse(stream_results())\n",
    "\n",
    "    # Non-streaming case\n",
    "    final_output = None\n",
    "    async for request_output in results_generator:\n",
    "        # if await request.is_disconnected():\n",
    "        #     # Abort the request if the client disconnects.\n",
    "        #     await engine.abort(request_id)\n",
    "        #     return Response(status_code=499)\n",
    "        final_output = request_output\n",
    "\n",
    "    assert final_output is not None\n",
    "    prompt = final_output.prompt\n",
    "    text_outputs = [prompt + output.text for output in final_output.outputs]\n",
    "    ret = {\"text\": text_outputs}\n",
    "    return JSONResponse(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71c312a-f004-44a1-aef1-c0bc7b3cb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class TestRequest(BaseModel):\n",
    "    prompt: str\n",
    "    stream: bool | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac777d44-1601-4803-8ecb-89f2335e2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_request = TestRequest(prompt=\"Finish learning how to\", stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e9ea1f-cd9a-4e82-a4eb-e5f398a1a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 01:55:21 async_llm_engine.py:371] Received request 3b30d8a0889f4d76a431c93cd012af99: prompt: 'Finish learning how to', sampling params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], ignore_eos=False, max_tokens=16, logprobs=None, skip_special_tokens=True), prompt token ids: None.\n",
      "INFO 10-03 01:55:21 llm_engine.py:616] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%\n",
      "INFO 10-03 01:55:22 async_llm_engine.py:111] Finished request 3b30d8a0889f4d76a431c93cd012af99.\n"
     ]
    }
   ],
   "source": [
    "t = await generate(test_request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
