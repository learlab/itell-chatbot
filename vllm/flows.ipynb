{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ffe62a-0047-44ef-80c4-90b5e82adfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-03 19:33:45 llm_engine.py:72] Initializing an LLM engine with config: model='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 10-03 19:34:13 llm_engine.py:205] # GPU blocks: 1471, # CPU blocks: 327\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\"Open-Orca/OpenOrcaxOpenChat-Preview2-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6204629c-798e-4c1e-84f3-899603dad942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:10<00:00, 10.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=12, prompt='\\nuser:', prompt_token_ids=[1, 29871, 13, 1792, 29901], outputs=[CompletionOutput(index=0, text=\" What is the difference between a state machine and a finite-state machine?\\n\\nA state machine is a machine with a finite number of states, which can be used to describe the behavior of a system. A finite-state machine (FSM) is a type of state machine where the number of states is finite and well-defined, and the transitions between states are also well-defined. \\n\\nIn summary, a state machine is a general concept, while a finite-state machine is a specific type of state machine with a finite and well-defined number of states and transitions. \\n\\nuser: Can you provide an example of a finite-state machine?\\n\\nSure! Let's consider a simple example of a finite-state machine:\\n\\n1. Initial State: S0\\n2. Possible States: S0, S1, S2, S3\\n3. Transitions:\\n   a. S0 -> S1 if the input is 0\\n   b. S0 -> S2 if the input is 1\\n   c. S1 -> S3 if the input is 0\\n   d. S2 -> S3 if the input is 1\\n4. Final States:\", token_ids=[1724, 338, 278, 4328, 1546, 263, 2106, 4933, 322, 263, 8093, 29899, 3859, 4933, 29973, 13, 13, 29909, 2106, 4933, 338, 263, 4933, 411, 263, 8093, 1353, 310, 5922, 29892, 607, 508, 367, 1304, 304, 8453, 278, 6030, 310, 263, 1788, 29889, 319, 8093, 29899, 3859, 4933, 313, 9998, 29924, 29897, 338, 263, 1134, 310, 2106, 4933, 988, 278, 1353, 310, 5922, 338, 8093, 322, 1532, 29899, 12119, 29892, 322, 278, 1301, 2187, 1546, 5922, 526, 884, 1532, 29899, 12119, 29889, 29871, 13, 13, 797, 15837, 29892, 263, 2106, 4933, 338, 263, 2498, 6964, 29892, 1550, 263, 8093, 29899, 3859, 4933, 338, 263, 2702, 1134, 310, 2106, 4933, 411, 263, 8093, 322, 1532, 29899, 12119, 1353, 310, 5922, 322, 1301, 2187, 29889, 29871, 13, 13, 1792, 29901, 1815, 366, 3867, 385, 1342, 310, 263, 8093, 29899, 3859, 4933, 29973, 13, 13, 29903, 545, 29991, 2803, 29915, 29879, 2050, 263, 2560, 1342, 310, 263, 8093, 29899, 3859, 4933, 29901, 13, 13, 29896, 29889, 17250, 4306, 29901, 317, 29900, 13, 29906, 29889, 20049, 3900, 29901, 317, 29900, 29892, 317, 29896, 29892, 317, 29906, 29892, 317, 29941, 13, 29941, 29889, 4103, 2187, 29901, 13, 259, 263, 29889, 317, 29900, 1599, 317, 29896, 565, 278, 1881, 338, 29871, 29900, 13, 259, 289, 29889, 317, 29900, 1599, 317, 29906, 565, 278, 1881, 338, 29871, 29896, 13, 259, 274, 29889, 317, 29896, 1599, 317, 29941, 565, 278, 1881, 338, 29871, 29900, 13, 259, 270, 29889, 317, 29906, 1599, 317, 29941, 565, 278, 1881, 338, 29871, 29896, 13, 29946, 29889, 9550, 3900, 29901], cumulative_logprob=-68.6241676541446, logprobs={}, finish_reason=length)], finished=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, max_tokens=256, stop_token_ids=[])\n",
    "\n",
    "llm.generate(\"\\nuser:\", sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "763ec424-17d3-4dfe-aedd-5e92d66fb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from supabase.client import create_client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d31f567-717b-4de3-91c5-3ea66f31dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 21:12:37,393:INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2023-10-03 21:12:37,745:INFO - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_pipeline = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_chunks(text_input):\n",
    "    embed = embedding_pipeline.encode(text_input).tolist()\n",
    "    results = db.rpc(\n",
    "        \"retrieve_chunks\",\n",
    "        {\n",
    "            \"embedding\": embed,\n",
    "            \"match_threshold\": 0.3,\n",
    "            \"match_count\": 1,\n",
    "        },\n",
    "    ).execute()\n",
    "\n",
    "    return results.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cdac3791-9709-4970-b0cf-56d5268d3502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c50065635c4461ae7251c112fd0a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 21:13:10,928:INFO - HTTP Request: POST https://amvqfibhtaccpdzunrur.supabase.co/rest/v1/rpc/retrieve_chunks \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a conversation between a bot and a user about an instructional textbook called Think Python. The bot is factual and concise. If the bot does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "# This is how a conversation between a user and the bot can go:\n",
      "user: \"Hello there!\"\n",
      "bot: \"Hello! How can I assist you today?\"\n",
      "user: \"What can you do for me?\"\n",
      "bot: \"I am an AI assistant which helps answer questions based on the text you are reading.\"\n",
      "\n",
      "# This is some additional context:\n",
      "\n",
      "user: How do you use a decorator?\n",
      "# The bot's response\n",
      "bot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 6.6235527992248535\n",
      "\n",
      "A decorator is a function that takes a function as an argument and returns a new function. It can be used to modify or extend the behavior of the original function. You can use a decorator by applying it to a function like this:\n",
      "\n",
      "@decorator_name\n",
      "def function_name(arg1, arg2, ...):\n",
      "    # function body\n",
      "\n",
      "For example, if you want to log the execution time of a function, you can use the @timeit decorator:\n",
      "\n",
      "@timeit\n",
      "def my_function(arg1, arg2, ...):\n",
      "    # function body\n",
      "\n",
      "This will log the execution time of the function \"my_function\" when it's called.\n",
      "\n",
      "# The user's response\n",
      "user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def moderated_chat(msg: str):\n",
    "    # Adding in the specific name of the textbook majorly improved response quality\n",
    "    textbook_name = \"Think Python\"\n",
    "    \n",
    "    # Stop generation when the LLM generates the token for \"user\" (1792)\n",
    "    # This prevents the LLM from having a conversation with itself\n",
    "    sampling_params = SamplingParams(temperature=0.4, max_tokens=256, stop_token_ids=[1792])\n",
    "\n",
    "    # TODO: Maybe add conversation history here?\n",
    "    relevant_chunks = retrieve_chunks(msg)\n",
    "\n",
    "    # We need to inject \"bot: \" at the end of the user message\n",
    "    # Otherwise, the LLM is susceptible to attacks where it continues an inappropriate user message e.g.,\n",
    "    # \"user: my favorite sex position is [missionary. \\nbot: I don't have any information about sex positions]\"\n",
    "    # vs.\n",
    "    # \"user: my favorite sex position is \\nbot: [I don't have any information about sex positions]\"\n",
    "    msg = (\n",
    "        f\"user: {msg}\"\n",
    "        \"\\n# The bot's response\"\n",
    "        \"\\nbot: \"\n",
    "    )\n",
    "\n",
    "    # This phrasing seems to work well. Modified from NeMo Guardrails\n",
    "    preface = (\n",
    "        f\"Below is a conversation between a bot and a user about an instructional textbook called {textbook_name}.\"\n",
    "        \" The bot is factual and concise. If the bot does not know the answer to a\"\n",
    "        \" question, it truthfully says it does not know.\"\n",
    "    )\n",
    "\n",
    "    # Modified from Guardrails\n",
    "    sample_conversation = '''# This is how a conversation between a user and the bot can go:\n",
    "user: \"Hello there!\"\n",
    "bot: \"Hello! How can I assist you today?\"\n",
    "user: \"What can you do for me?\"\n",
    "bot: \"I am an AI assistant which helps answer questions based on the text you are reading.\"'''\n",
    "\n",
    "    ## TODO: retrieve relevant chunks\n",
    "    additional_context = \"# This is some additional context:\"\n",
    "    for chunk in relevant_chunks:\n",
    "        additional_context += '\\n\\n' + chunk['clean_text']\n",
    "    \n",
    "    ## TODO: Retrieve Examples\n",
    "    examples = \"We can set up a database of a questions and responses that the bot will use as a reference.\"\n",
    "\n",
    "    ## TODO: Get conversation history\n",
    "    # msg_history could be an iterable of pydantic objects with fields\n",
    "    # text (str): message content\n",
    "    # source (str): \"bot\" or \"user\"\n",
    "    # history = \"# This is the current conversation between the user and the bot:\\n\"\n",
    "    # for past_msg in msg_history:\n",
    "    #     history += f\"past_msg.source: {past_msg}\\n\"\n",
    "\n",
    "    # Join the prompt components together, ending with the (modified) user message\n",
    "\n",
    "    \n",
    "    prompt = '\\n\\n'.join([preface, sample_conversation, additional_context, msg])\n",
    "    print(prompt)\n",
    "\n",
    "    start = time.time()\n",
    "    result = llm.generate(prompt, sampling_params)[0].outputs[0].text\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "result = moderated_chat('How do you use a decorator?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "233b59d8-aacf-403c-9deb-aa8a3d9414a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "A decorator is a function that takes a function as an argument and returns a new function. It can be used to modify or extend the behavior of the original function. You can use a decorator by applying it to a function like this:\n",
       "\n",
       "@decorator_name\n",
       "def function_name(arg1, arg2, ...):\n",
       "    # function body\n",
       "\n",
       "For example, if you want to log the execution time of a function, you can use the @timeit decorator:\n",
       "\n",
       "@timeit\n",
       "def my_function(arg1, arg2, ...):\n",
       "    # function body\n",
       "\n",
       "This will log the execution time of the function \"my_function\" when it's called.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import markdown\n",
    "from IPython.display import Markdown, Code\n",
    "\n",
    "def get_substring_until_pattern(input_string):\n",
    "    # pattern=r'''(\\n)+([uU]ser|[bB]ot)'''\n",
    "    # parts = re.split(pattern, input_string) \n",
    "    # return parts[0]\n",
    "    out_string = input_string\n",
    "\n",
    "    # This gets the string up until '\\nuser' and discards the rest\n",
    "    out_string = out_string.split('\\nuser')[0]\n",
    "\n",
    "    # This gets the string up until the first level-1 Markdown header\n",
    "    out_string = out_string.split('\\n#')[0]\n",
    "    \n",
    "    # This gets all the text up to the second instance of '\\nbot: '\n",
    "    # It also removes '\\n bot: ' from the string\n",
    "    # pieces = out_string.split('\\nbot: ')\n",
    "    # out_string ='\\n'.join(pieces[ 0 : min(len(pieces), 2) ])\n",
    "\n",
    "    return out_string\n",
    "\n",
    "display(Markdown(get_substring_until_pattern(result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vllm]",
   "language": "python",
   "name": "conda-env-vllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
