{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16a0e11-6fdb-48a4-b33b-f199891b71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 16:57:36,843:INFO - HTTP Request: GET https://amvqfibhtaccpdzunrur.supabase.co/rest/v1/subsections?select=%2A \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "url = os.getenv('SUPABASE_URL')\n",
    "key = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "from supabase import create_client, Client\n",
    "supabase = create_client(supabase_url=url, supabase_key=key)\n",
    "subsections_df = pd.DataFrame(supabase.table('subsections').select('*').execute().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01bcce0a-d536-471c-ac89-10c2eabbcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_dict = {'inference' : '''\n",
    "1.Inference: Inferential, or implicit, questions are answered by interpreting clues from part of the\n",
    "text to figure something out. Students need to be able to answer inferential questions to see if they\n",
    "are   understanding   the   meaning   behind   certain   events/character's   feelings.   Need   to   speak   about\n",
    "making   connections   between   segments   of   texts   to   fill   in   gaps   related   to   comprehension.\n",
    "Inferences are implicit and are based on using background knowledge or text information to make\n",
    "conclusions.\n",
    "''',\n",
    "                   'reflection' : '''\n",
    "2.Reflection:   A   reflection   question   is   what   we   call   any   question   that   makes   a   student   look   back\n",
    "over   what   or   how   they   have   learned.   Reflection   questions   often   assess   metacognitive   skills,\n",
    "otherwise known as thinking about how we think and learn. Relevant elaborations expand given\n",
    "information.   Relevant   elaborations   add   information   that   supports   or   clarifies   conclusions.\n",
    "''',\n",
    "                   'elaboration' : '''                   \n",
    "3.Elaboration:   These   questions   help   to   extend   and   broaden   the   importance   of   the   meaning. \n",
    "Learners can elaborate on the question making it more personal to them. Make sure that these are\n",
    "related to a readers' background knowledge and personal experience.\n",
    "'''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac7705b-f1fb-4992-b231-a668e346a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 16:57:37 llm_engine.py:72] Initializing an LLM engine with config: model='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer='Open-Orca/OpenOrcaxOpenChat-Preview2-13B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2023-11-29 16:57:38,749:INFO - Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-11-29 16:57:38,751:INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "2023-11-29 16:57:39,906:INFO - Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-11-29 16:57:39,908:INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "2023-11-29 16:57:39,910:INFO - Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "2023-11-29 16:57:39,911:INFO - Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 16:58:06 llm_engine.py:207] # GPU blocks: 1471, # CPU blocks: 327\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\"Open-Orca/OpenOrcaxOpenChat-Preview2-13B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46028778-c46b-4991-a490-ab341dee064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsections_df = subsections_df[['subsection_id', 'clean_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8794ef29-ab1c-4f18-886b-88271d8565db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "topic = 'computer programming in python'\n",
    "question_types = list(definition_dict.keys())\n",
    "excerpt = subsections_df.iloc[23]['clean_text']\n",
    "\n",
    "def generate_qa_output(excerpt, question_type):\n",
    "    type_def = definition_dict[question_type]\n",
    "    preface = f'''\n",
    "    I will take the following quoted text about {topic} and create a short-answer question with only one correct answer.\n",
    "    I will also include the correct answer. \n",
    "    The following is the text on the aforementioned topic:\n",
    "    \n",
    "    [START OF EXCERPT]\n",
    "    {excerpt}\n",
    "    [END OF EXCERPT]\n",
    "    '''\n",
    "    \n",
    "    command = f'''\n",
    "    Here is a short-answer question for the text above. This will be a {question_type} question. Here is a definition of that type of question: {type_def}\n",
    "    First, I will write the type of question. Then, I will write the question. Finally, I will write the correct answer. At the end I will write [END OF QUESTION].\n",
    "    '''\n",
    "    \n",
    "    input = preface + command\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=248)\n",
    "    response = llm.generate(input, sampling_params, use_tqdm=False)[0].outputs[0].text.split('END OF QUESTION')[0].strip('[').strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9956d02-4c88-4edc-8efa-93b971762c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [09:55<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reflection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [09:55<00:00, 10.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaboration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [09:54<00:00, 10.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "for question_type in question_types:\n",
    "    print(question_type)\n",
    "    subsections_df[question_type] = subsections_df['clean_text'].progress_apply(lambda x: generate_qa_output(x, question_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad28a5ae-66fd-42c5-b2db-faef4a77826e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Question: Reflection\n",
      "    \n",
      "    Question: Look back at the information provided in the text. Based on the examples given, what is the difference between the variables message, n, and pi?\n",
      "    \n",
      "    Correct Answer: The variable message is a string, n is an integer, and pi is an approximate value.\n"
     ]
    }
   ],
   "source": [
    "print(subsections_df.iloc[30]['reflection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11aaf1-9cb9-4b0a-86b9-82b34269f675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wes_main]",
   "language": "python",
   "name": "conda-env-wes_main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
